<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>TFNW Workshop -- BYOB</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="css/reveal.css"/>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="css/theme/moon.css" id="theme">
  <!-- If the query includes 'print-pdf', include the PDF print sheet -->
  <script>
    if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
  </script>
  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">TFNW Workshop -- BYOB</h1>
    <h3 class="date"></h3>
</section>

<section id="byob-brain-not-beer" class="slide level2">
<h1><a href="http://hobsonlane.com/talks/Neural-Nets-Demystified.html">BYOB</a> (<strong>B</strong>rain not <strong>B</strong>eer)</h1>
<p>See the <a href="http://hobsonlane.com/talks/TFNW-BYO-Brain.html">reveal.js slides</a> for the latest</p>
</section>
<section id="intro" class="slide level2">
<h1><a href="http://hobsonlane.com/talks/Neural-Nets-Demystified.html">Intro</a></h1>
<ol type="1">
<li class="fragment">Neurons</li>
<li class="fragment">Layers</li>
<li class="fragment">Code</li>
</ol>
<div>
<aside class="notes">
      <p>
First I’ll show you a simple example (predicting Portland Weather).
</p>
      <p>
Then I’ll show you how to play around at the frontier of the state of the art
</p>
    <p>
Thoughts about the upcoming <a href="http://www.meetup.com/Portland-Data-Science-Group/events/222322211/">PDX Data Science Meetup</a>
<p>
    <p>
<a href="/Data-Science-Meetup--Neural-Nets-Demystified/">&quot;Neural Nets Demystified.&quot;</a>
<p>
  </aside>
</div>
</section>
<section class="slide level2">

</section>
<section><section id="neurons" class="titleslide slide level1"><h1>Neurons</h1></section><section id="brain-research" class="slide level2">
<h1>Brain Research</h1>
<p>Neuroscientists simulated a <a href="http://openbrain.org">whole brain</a></p>
<div class="fragment">
<p>... of a nematode worm (C Elegans)</p>
<p>~300 neurons ~200 in central nervous system</p>
</div>
</section><section class="slide level2">

<figure>
<img src="image/BYOB-CelegansGoldsteinLabUNC.jpg" alt="C Elegans, perhaps the most photographed organism of all time" /><figcaption>C Elegans, perhaps the most photographed organism of all time</figcaption>
</figure>
</section><section class="slide level2">

<figure>
<img src="image/BYOB-CelegansGoldsteinLabUNC.jpg" alt="C Elegans&#39; brain is shaped like a donut or our pharynx" /><figcaption>C Elegans' brain is shaped like a donut or our pharynx</figcaption>
</figure>
</section><section id="animal-intelligence" class="slide level2">
<h1>Animal Intelligence</h1>
<p>Artificial brains aren't at all like human brains,</p>
<div class="fragment">
<p>or even a worm brain</p>
<p>Neuron simulations are broad abstractions - pulses in time not modeled - chemistry - neuron internal feedback loops</p>
</div>
</section><section id="getting-abstractions-right-reveals-the-truth" class="slide level2">
<h1>Getting abstractions right reveals the truth</h1>
<ul>
<li>In the 18th century, the planets and stars were nested spheres</li>
<li>Copernicus revealed put the Earth at the center</li>
<li>Now we can forecast the position of satellites and planets to centimeters</li>
</ul>
</section><section id="math" class="slide level2">
<h1>Math</h1>
<ul>
<li>You don't need to know Linear Algebra, just...
<ul>
<li>multiply</li>
<li>add</li>
<li>check thresholds</li>
</ul></li>
<li>Equation/code is short (in python)</li>
<li>100s of neurons
<ul>
<li>Not billions</li>
</ul></li>
<li>Train for minutes
<ul>
<li>Not decades</li>
</ul></li>
</ul>
</section><section id="mcculloch-pitts-neuron" class="slide level2">
<h1>McCulloch Pitts Neuron&quot;</h1>
<p>Modeled after biological neurons. Can be combined to perform any logical or mathematical operation.</p>
<p>Binary output: 0 or +1 Any Number of <strong>binary</strong> inputs Inhibitory input with &quot;veto&quot; power</p>
</section><section id="rosenblatts-perceptron" class="slide level2">
<h1>Rosenblatt's Perceptron</h1>
<p>Designed to be &quot;trainable&quot; Rosenblatt provided a training algorithm</p>
<p>Binary output: <strong>-1</strong> or +1 Any number of real inputs Threshold = 0 Weights and inputs can be real-valued</p>
</section><section id="modern-neuron" class="slide level2">
<h1>Modern Neuron</h1>
</section><section id="activation-functions" class="slide level2">
<h1>Activation functions</h1>
<ul>
<li>sigmoid</li>
<li>saturation</li>
<li>threshold</li>
<li>linear</li>
<li>sync</li>
<li>tanh</li>
</ul>
</section><section id="modern" class="slide level2">
<h1>Modern</h1>
</section><section id="layers" class="slide level2">
<h1>Layers</h1>
<ul>
<li>Many layers (6+ for)</li>
<li>Many neurons/layer</li>
<li>Sophisticated Connection Architectures
<ul>
<li>fully-connected</li>
<li>convolutional</li>
<li>recursive</li>
<li>sparse</li>
<li>random</li>
<li>scale-free</li>
</ul></li>
</ul>
</section><section id="neural-nets-were-made-for-..." class="slide level2">
<h1>Neural Nets were &quot;made&quot; for ...</h1>
</section><section id="hyperdimensionality" class="slide level2">
<h1>Hyperdimensionality</h1>
<ul>
<li>Images (object recognition)</li>
<li>Sound (speech recognition)</li>
<li>Time series (weather, finance, election prediction)</li>
</ul>
</section><section id="pattern-recognition" class="slide level2">
<h1>Pattern Recognition</h1>
<ul>
<li class="fragment">Prediction</li>
<li class="fragment">Segmentation (sound, image)</li>
<li class="fragment">Feature detection</li>
<li class="fragment">Fraud detection</li>
<li class="fragment">Intrusion detection</li>
<li class="fragment">Game cheating detection . . .</li>
</ul>
<p>But often they can produce useful features that seemingly don't make sense</p>
<div class="fragment">
<p>except for images</p>
</div>
</section><section id="neural-nets-help-when-..." class="slide level2">
<h1>Neural Nets help when ...</h1>
</section><section id="you-dont-know-what-to-look-for-feature-engineering" class="slide level2">
<h1>You don't know what to look for (feature engineering)</h1>
<ul>
<li>FFT</li>
<li>DCT</li>
<li>Wavelets</li>
<li>PCA/SVD</li>
<li>RF</li>
<li>Statistics (mean, std, diff, polynomial)</li>
<li>LPF/BPF/HPF</li>
<li>Resampling/Interpolation/Extrapolation</li>
</ul>
<p>all fail</p>
</section><section id="and-when-..." class="slide level2">
<h1>And when ...</h1>
</section><section id="conventional-control-laws-fail" class="slide level2">
<h1>Conventional control laws fail</h1>
<ul>
<li>shooting a basketball</li>
<li>kicking a soccer ball</li>
<li>stabilizing an inverted pendulum</li>
<li>helicopter stunts</li>
</ul>
</section><section id="neural-nets-can-help-invert-physics-models" class="slide level2">
<h1>Neural Nets can help invert &quot;Physics&quot; models</h1>
<ul>
<li>Infer reflectance despite shadow/glare/haze</li>
<li>2-D image -&gt; 3-D object</li>
<li>When direct measurement of 3-D not possible
<ul>
<li>stereoscopic vision</li>
<li>structured light</li>
<li>lidar</li>
<li>radar</li>
<li>sonar</li>
<li>Kinect or RealSense</li>
</ul></li>
</ul>
</section><section id="neural-nets-can-see-through-structure-noise" class="slide level2">
<h1>Neural Nets can &quot;see&quot; through structure noise</h1>
<p>Both images and sound often suffer from</p>
<ul>
<li>occlusion</li>
<li>obsucration/haze/fog/fade</li>
<li>rotation/translation/warping</li>
</ul>
</section><section id="neural-nets-need-data-and-power" class="slide level2">
<h1>Neural Nets need data and power</h1>
<ul>
<li>Lots of examples to learn from</li>
<li>CPU/GPU cycles to burn
<ul>
<li>Google speech recognition doesn't run on your phone...yet</li>
</ul></li>
</ul>
</section><section class="slide level2">

</section></section>
<section><section id="layers-1" class="titleslide slide level1"><h1>Layers</h1></section><section id="classification" class="slide level2">
<h1>Classification</h1>
<p>The most basic ML task is classification</p>
<p>In NN lingo, this is called &quot;association&quot;</p>
<p>So lets predict &quot;rain&quot; (1) &quot;no rain&quot; (0) for PDX tomorrow</p>
</section><section id="supervised-learning" class="slide level2">
<h1>Supervised Learning</h1>
<p>We have historical &quot;examples&quot; of rain and shine</p>
<p><a href="http://wunderground.org">Weather Underground</a></p>
<p>Since we know the classification (training set)...</p>
<p>Supervised classification (association)</p>
</section><section id="rain-shine-partly-cloudy" class="slide level2">
<h1>Rain, Shine, Partly-Cloudy ?</h1>
<p>Wunderground lists several possible &quot;conditions&quot; or classes</p>
<p>If we wanted to predict them all</p>
<p>We would just make a binary classifier for each one</p>
<p>All classification problems can be reduced a binary classification</p>
</section><section id="perceptron" class="slide level2">
<h1><a href="https://en.wikipedia.org/wiki/Perceptron"><em>Perceptron</em></a></h1>
<p>Sounds mysterious, like a &quot;flux capacitor&quot; or something...</p>
<p>It's just a multiply and threshold check:</p>
<p>{% highlight python %} if (weights * inputs) &gt; 0: output = 1 else: output = 0 {% endhighlight %}</p>
</section><section id="perceptron-1" class="slide level2">
<h1>Perceptron</h1>
<p>(Diagram of a perceptron)</p>
</section><section id="need-something-a-little-better" class="slide level2">
<h1>Need something a little better</h1>
<p>Works fine for &quot;using&quot; (<em><a href="https://en.wikipedia.org/wiki/Activation_function">activating</a></em>) your NN</p>
<p>But for learning (<em><a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a></em>) you need it to be predictable...</p>
<ul>
<li>doesn't change direction on you: <em><a href="https://en.wikipedia.org/wiki/Monotonic_function">monotonic</a></em></li>
<li>doesn't jump around: <em><a href="https://en.wikipedia.org/wiki/Smoothness">smooth</a></em></li>
</ul>
</section><section id="sigmoid" class="slide level2">
<h1><a href="https://en.wikipedia.org/wiki/Perceptron"><em>Sigmoid</em></a></h1>
<p>Again, sounds mysterious... like a transcendental function</p>
<p>It is a transcendental function, but the word just means</p>
<p>Curved, smooth like the letter &quot;C&quot;</p>
</section><section id="what-greek-letter-do-you-think-of-when-i-say-sigma" class="slide level2">
<h1>What Greek letter do you think of when I say &quot;Sigma&quot;?</h1>
</section><section id="σ" class="slide level2">
<h1>&quot;Σ&quot;</h1>
<p>What Roman (English) character?</p>
<ul>
<li>&quot;E&quot;?</li>
<li>&quot;S&quot;?</li>
<li>&quot;C&quot;?</li>
</ul>
</section><section id="sigma" class="slide level2">
<h1><a href="https://en.wikipedia.org/wiki/Sigma">Sigma</a></h1>
<p>You didn't know this was a Latin/Greek class, did you...</p>
<p>Σ (uppercase) σ (lowercase) ς (last letter in word) c (alternatively)</p>
</section><section class="slide level2">

<p>Most English speakers think of an &quot;S&quot;</p>
<p>when they hear &quot;Sigma&quot;.</p>
<p>So the meaning has evolved to mean S-shaped.</p>
</section><section id="shaped-like-an-s" class="slide level2">
<h1>Shaped like an &quot;S&quot;</h1>
<p>The trainer (<em>(backpropagator)[https://en.wikipedia.org/wiki/Backpropagation]</em>) can predict the change in <code>weights</code> required Wants to nudge the <code>output</code> closer to the <code>target</code></p>
<p><code>target</code>: known classification for training examples <code>output</code>: predicted classification your network spits out</p>
</section><section id="but-just-a-nudge." class="slide level2">
<h1>But just a nudge.</h1>
<p>Don't get greedy and push all the way to the answer Because your linear sloper predictions are wrong And there may be nonlinear interactions between the weights (multiply layers)</p>
<p>So set the learning rate () to somthething less than 1 the portion of the predicted nudge you want to &quot;dial back&quot; to</p>
</section><section class="slide level2">

</section></section>
<section><section id="code" class="titleslide slide level1"><h1>Code</h1></section><section id="example-predict-rain-in-portland" class="slide level2">
<h1>Example: Predict Rain in Portland</h1>
<ul>
<li>PyBrain</li>
<li>pug-ann (helper functions TBD PyBrain2)</li>
</ul>
</section><section id="get-historical-weather-for-portland-then-..." class="slide level2">
<h1>Get historical weather for Portland then ...</h1>
<ol type="1">
<li>Backpropagate: train a perceptron</li>
<li>Activate: predict the weather for tomorrow!</li>
</ol>
</section><section id="nn-advantages" class="slide level2">
<h1>NN Advantages</h1>
<ul>
<li>Easy
<ul>
<li>No math!</li>
<li>No tuning!</li>
<li>Just plug and chug.</li>
</ul></li>
<li>General
<ul>
<li>One model can apply to many problems</li>
</ul></li>
<li>Advanced
<ul>
<li>They often beat all other &quot;tuned&quot; approaches</li>
</ul></li>
</ul>
</section><section id="disadvantage-1-slow-to-learn" class="slide level2">
<h1>Disadvantage #1: Slow to Learn</h1>
<ul>
<li>cubic to learn
<ul>
<li>quadratic to activate</li>
</ul></li>
</ul>
</section><section id="example" class="slide level2">
<h1>Example</h1>
<ul>
<li>24+ hr for complex Kaggle example on laptop</li>
<li>90x30x20x10 ~= 1M DOF
<ul>
<li>90 input dimensions (regressors)</li>
<li>30 nodes for <em>hidden layer</em> 1</li>
<li>20 nodes for <em>hidden layer</em> 2</li>
<li>10 output dimensions (predicted values)</li>
</ul></li>
</ul>
</section><section id="disadvantage-2-they-dont-often-scale-difficult-to-parallelize" class="slide level2">
<h1>Disadvantage #2: They don't often scale (difficult to parallelize)</h1>
<ul>
<li>Fully-connected NNs can't be <em>easily</em> hyper-parallelized (GPU)
<ul>
<li>Large matrix multiplications</li>
<li>Layers depend on all elements of previous layers</li>
</ul></li>
</ul>
</section><section id="scaling-workaround" class="slide level2">
<h1>Scaling Workaround</h1>
<p>At Kaggle workshop we discussed paralleling linear algebra</p>
<ul>
<li>Split matrices up and work on &quot;tiles&quot;</li>
<li>Theano, <a href="">Keras</a> for python</li>
<li><a href="http://icl.cs.utk.edu/news_pub/submissions/plasma-scidac09.pdf">PLASMA</a> for BLAS</li>
</ul>
</section><section id="scaling-workaround-limitations" class="slide level2">
<h1>Scaling Workaround Limitations</h1>
<p>But tiles must be shared/consolidated and theirs redundancy</p>
<ul>
<li>Data flow: Main -&gt; CPU -&gt; GPU -&gt; GPU cache (and back)</li>
<li>Data com (RAM xfer) is limiting</li>
<li>Data RAM size (at each stage) is limiting</li>
<li><a href="http://icl.cs.utk.edu/news_pub/submissions/plasma-scidac09.pdf">Each GPU is equivalent to 16 core node</a></li>
</ul>
</section><section id="disadvantage-3-they-overfit" class="slide level2">
<h1>Disadvantage #3: They overfit</h1>
<ul>
<li>Too manu nodes = overfitting</li>
</ul>
</section><section id="what-is-the-big-o" class="slide level2">
<h1>What is the big O?</h1>
<ul>
<li>Degrees of freedom grow with number of nodes &amp; layers</li>
<li>Each layer's nodes connected to each previous layer's</li>
<li>That a lot of wasted &quot;freedom&quot;</li>
<li>Many weights are randomly zeroed/ignored (Random Dropout)</li>
</ul>
</section><section id="on2-to-activate" class="slide level2">
<h1>O(N^2) to activate</h1>
</section><section id="on3-to-learn" class="slide level2">
<h1>O(N^3) to learn</h1>
</section><section id="not-so-fast-big-o..." class="slide level2">
<h1>Not so fast, big O...</h1>
<p>{% highlight python %} &gt;&gt;&gt; np.prod([30, 20, 10]) 6000 &gt;&gt;&gt; np.sum([30, 20, 10])**2 3600 {% endhighlight %}</p>
</section><section class="slide level2">

<p>Rule of thumb</p>
<p>NOT <code>N**2</code></p>
<p>But <code>M * N**2</code></p>
<p>N: number of nodes M: number of layers</p>
</section><section class="slide level2">

<p><code>assert(M * N**2 &lt; len(training_set) / 10.)</code></p>
<p>I'm serious... put this into your code. I wasted a lot of time training models for Kaggle that was overfit.</p>
</section><section class="slide level2">

<p>You do need to know math!</p>
<ul>
<li>To imprint your net with the structure (math) of the problem
<ul>
<li>Feature analysis or transformation (conventional ML)</li>
<li>Choosing the activation function and segmenting your NN</li>
</ul></li>
<li>Prune and evolve your NN</li>
</ul>
</section><section class="slide level2">

<p>This is a virtuous cycle!</p>
<ul>
<li>More structure (no longer fully connected)
<ul>
<li>Each independent path (segment) is parallelizable!</li>
</ul></li>
<li>Automatic tuning, pruning, evolving is all parallelizable!
<ul>
<li>Just train each NN separately</li>
<li>Check back in with Prefrontal to &quot;compete&quot;</li>
</ul></li>
</ul>
</section><section class="slide level2">

<p>Structure you can play with (textbook)</p>
<ul>
<li>limit connections</li>
</ul>
<p>jargon: <em>receptive fields</em></p>
<ul>
<li>limit weights</li>
</ul>
<p>jargon: <em>weight sharing</em></p>
<p>All the rage: <em>convolutional networks</em></p>
</section><section class="slide level2">

<p>Unconventional structure to play with</p>
<p>New ideas, no jargon yet, just crackpot names</p>
<ul>
<li>limit weight ranges (e.g. -1 to 1, 0 to 1, etc)</li>
<li>weight &quot;snap to grid&quot; (snap learning)</li>
</ul>
</section><section class="slide level2">

<p>Joke: &quot;What's the difference between a scientist and a crackpot?&quot;</p>
</section><section class="slide level2">

<p>Ans: &quot;P-value&quot;</p>
<ul>
<li>High-<strong>P</strong>robability null hypothesis</li>
<li>Not <strong>P</strong>ublished</li>
<li>Not <strong>P</strong>eer-reviewed</li>
<li>No <strong>P</strong>yPi <strong>p</strong>ackage</li>
</ul>
<p>I'm a crackpot!</p>
</section><section class="slide level2">

<p>Resources</p>
<ul>
<li><a href="http://keras.io/">keras.io</a>: Scalable Python NNs</li>
<li><a href="http://hagan.okstate.edu/NNDesign.pdf">Neural Network Design</a>: Free NN Textbook!</li>
<li><a href="https://github.com/hobson/pug-ann">pug-ann</a>: Helpers for PyBrain and Wunderground</li>
<li><a href="https://github.com/pybrain2/pybrain2">PyBrain2</a>: We're working on it</li>
</ul>
</section><section class="slide level2">

<p>Code highlighting test</p>
<p>{% highlight javascript %} function linkify( selector ) { if( supports3DTransforms ) {</p>
<pre><code>var nodes = document.querySelectorAll( selector );

for( var i = 0, len = nodes.length; i &amp;lt; len; i++ ) {
  var node = nodes[i];

  if( !node.className ) {
    node.className += &#39; roll&#39;;
  }
}</code></pre>
<p>} } {% endhighlight %}</p>
</section></section>
    </div>
  </div>


  <script src="lib/js/head.min.js"></script>
  <script src="js/reveal.js"></script>

  <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,         // Display controls in the bottom right corner
        progress: true,         // Display a presentation progress bar
        history: true,          // Push each slide change to the browser history
        center: false,                       // Vertical centering of slides
        maxScale: 1.5,                  // Bounds for smallest/largest possible content scale
        slideNumber: false,                // Display the page number of the current slide
        theme: 'moon', // available themes are in /css/theme
        transition: 'fade', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
//          { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
//          { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
]});
    </script>
    </body>
</html>
