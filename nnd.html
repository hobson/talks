<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <meta name="author" content="Hobson Lane" />
    <meta name="dcterms.date" content="2015-05-19" />
    <title>Neural Nets Demystified</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
  <div class="reveal">
    <div class="slides">
      <section>
      <h2 id="neural-nets-demystified">Neural Nets Demystified</h2>
      <ol>
        <li>Demystify</li>
        <li>Dig Deeper</li>
      </ol>
      <aside class="notes">
      <p>First I’ll show you a simple example (predicting Portland Weather).</p>
      <p>Then I’ll show you how to play around at the frontier of the state of the art</p>
<ul>
  <li>Thoughts about the upcoming <a href="http://www.meetup.com/Portland-Data-Science-Group/events/222322211/">PDX Data Science Meetup</a></li>
  <li><a href="/Data-Science-Meetup--Neural-Nets-Demystified/">“Neural Nets Demystified.”</a></li>
</ul>

</aside>
</section>
<section>


<h2 id="classification">Classification</h2>

<p>The most basic ML task is classification</p>

<p>In NN lingo, this is called “association”</p>

<p>So lets predict “rain” (1) “no rain” (0) for PDX tomorrow</p>

</section><section>

<h2 id="supervised-learning">Supervised Learning</h2>

<p>We have historical “examples” of rain and shine</p>

<p><a href="http://wunderground.org">Weather Underground</a></p>

<p>Since we know the classification (training set)…</p>

<p>Supervised classification (association)</p>

</section><section>

<h2 id="rain-shine-partly-cloudy-">Rain, Shine, Partly-Cloudy ?</h2>

<p>Wunderground lists several possible “conditions” or classes</p>

<p>If we wanted to predict them all </p>

<p>We would just make a binary classifier for each one</p>

<p>All classification problems can be reduced a binary classification</p>

</section><section>

<h2 id="perceptronhttpsenwikipediaorgwikiperceptron"><a href="https://en.wikipedia.org/wiki/Perceptron"><em>Perceptron</em></a></h2>

<p>Sounds mysterious, like a “flux capacitor” or something…</p>

<p>It’s just a multiply and threshold check:</p>

<pre><code class="hightlight" data-lang="python">if (weights * inputs) > 0:
    output = 1
else:
    output = 0
</code></pre>

</section><section>

<h2 id="perceptron">Perceptron</h2>

<p>(Diagram of a perceptron)</p>

</section><section>

<h2 id="need-something-a-little-better">Need something a little better</h2>

<p>Works fine for “using” (<em><a href="https://en.wikipedia.org/wiki/Activation_function">activating</a></em>) your NN</p>

<p>But for learning (<em><a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a></em>) you need it to be predictable…</p>

<ul>
  <li>doesn’t change direction on you: <em><a href="https://en.wikipedia.org/wiki/Monotonic_function">monotonic</a></em></li>
  <li>doesn’t jump around: <em><a href="https://en.wikipedia.org/wiki/Smoothness">smooth</a></em></li>
</ul>

</section><section>

<h2 id="sigmoidhttpsenwikipediaorgwikiperceptron"><a href="https://en.wikipedia.org/wiki/Perceptron"><em>Sigmoid</em></a></h2>

<p>Again, sounds mysterious… like a transcendental function</p>

<p>It is a transcendental function, but the word just means</p>

<p>Curved, smooth like the letter “C”</p>

</section><section>

<h2 id="what-greek-letter-do-you-think-of-when-i-say-sigma">What Greek letter do you think of when I say “Sigma”?</h2>

<h3 id="section">“Σ”</h3>

<p>What Roman (English) character?</p>

<ul>
  <li>“E”?</li>
  <li>“S”?</li>
  <li>“C”?</li>
</ul>

</section><section>

<h2 id="sigmahttpsenwikipediaorgwikisigma"><a href="https://en.wikipedia.org/wiki/Sigma">Sigma</a></h2>

<p>You didn’t know this was a Latin/Greek class, did you…</p>

<p>Σ (uppercase)
σ (lowercase)
ς (last letter in word)
c (alternatively)</p>

<p>Most English speakers think of an “S” when they hear “Sigma” you think of an S.
So the meaning has evolved to mean S-shaped.</p>

</section><section>

<p>That’s what we want, something smooth, shaped like an “S”</p>

<p>The trainer (<em>(backpropagator)[https://en.wikipedia.org/wiki/Backpropagation]</em>) can predict the change in <code>weights</code> required
Wants to nudge the <code>output</code> closer to the <code>target</code></p>

<p><code>target</code>: known classification for training examples
<code>output</code>: predicted classification your network spits out</p>

</section><section>

<h2 id="but-just-a-nudge">But just a nudge.</h2>

<p>Don’t get greedy and push all the way to the answer
Because your linear sloper predictions are wrong
And there may be nonlinear interactions between the weights (multiply layers)</p>

<p>So set the learning rate (\alpha) to somthething less than 1
the portion of the predicted nudge you want to “dial back” to</p>

</section><section>

<h2 id="example-predict-rain-in-portland">Example: Predict Rain in Portland</h2>

<ul>
  <li>PyBrain</li>
  <li>pug-ann (helper functions TBD PyBrain2)</li>
</ul>

</section><section>

<p>Get historical weather for Portland then …</p>

<ol>
  <li>Backpropagate: train a perceptron</li>
  <li>Activate: predict the weather for tomorrow!</li>
</ol>

</section><section>

<p>NN Advantages</p>

<ul>
  <li>Easy
    <ul>
      <li>No math!</li>
      <li>No tuning!</li>
      <li>Just plug and chug.</li>
    </ul>
  </li>
  <li>General
    <ul>
      <li>One model can apply to many problems</li>
    </ul>
  </li>
  <li>Advanced
    <ul>
      <li>They often beat all other “tuned” approaches</li>
    </ul>
  </li>
</ul>

</section><section>

<p>Disadvantage #1: Slow training</p>

<ul>
  <li>24+ hr for complex Kaggle example on laptop</li>
  <li>90x30x20x10 model degrees freedom
    <ul>
      <li>90 input dimensions (regressors)</li>
      <li>30 nodes for <em>hidden layer</em> 1</li>
      <li>20 nodes for <em>hidden layer</em> 2</li>
      <li>10 output dimensions (predicted values)</li>
    </ul>
  </li>
</ul>

</section><section>

<p>Disadvantage #2: They don’t scale (unparallelizable)</p>

<ul>
  <li>Fully-connected NNs can’t be <em>easily</em> hyper-parallelized (GPU)
    <ul>
      <li>Large matrix multiplications</li>
      <li>Layers depend on all elements of previous layers</li>
    </ul>
  </li>
</ul>

</section><section>

<p>Scaling Workaround</p>

<p>At Kaggle workshop we discussed paralleling linear algebra</p>

<ul>
  <li>Split matrices up and work on “tiles”</li>
  <li>Theano, <a href="">Keras</a> for python</li>
  <li><a href="http://icl.cs.utk.edu/news_pub/submissions/plasma-scidac09.pdf">PLASMA</a> for BLAS</li>
</ul>

</section><section>

<p>Scaling Workaround Limitations</p>

<p>But tiles must be shared/consolidated and theirs redundancy</p>

<ul>
  <li>Data flow: Main -&gt; CPU -&gt; GPU -&gt; GPU cache (and back)</li>
  <li>Data com (RAM xfer) is limiting</li>
  <li>Data RAM size (at each stage) is limiting </li>
  <li><a href="http://icl.cs.utk.edu/news_pub/submissions/plasma-scidac09.pdf">Each GPU is equivalent to 16 core node</a></li>
</ul>

</section><section>

<p>Disadvantage #3: They overfit</p>

<ul>
  <li>Too manu nodes = overfitting</li>
</ul>

</section><section>

<p>What is the big O?</p>

<ul>
  <li>Degrees of freedom grow with number of nodes &amp; layers</li>
  <li>Each layer’s nodes connected to each previous layer’s</li>
  <li>That a lot of wasted “freedom”</li>
</ul>

<h2 id="on2">O(N^2)</h2>

</section><section>

<p>Not so fast, big O…</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="mi">6000</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
<span class="mi">3600</span></code></pre></div>

</section><section>

<p>Rule of thumb </p>

<p>NOT <code>N**2</code> </p>

<p>But <code>M * N**2</code></p>

<p>N: number of nodes
M: number of layers</p>

</section><section>

<p><code>assert(M * N**2 &lt; len(training_set) / 10.)</code></p>

<p>I’m serious… put this into your code.
I wasted a lot of time training models for Kaggle that overfitted.</p>

</section><section>

<p>You do need to know math!</p>

<ul>
  <li>To imprint your net with the structure (math) of the problem
    <ul>
      <li>Feature analysis or transformation (conventional ML)</li>
      <li>Choosing the activation function and segmenting your NN</li>
    </ul>
  </li>
  <li>Prune and evolve your NN</li>
</ul>

</section><section>

<p>This is a virtuous cycle!</p>

<ul>
  <li>More structure (no longer fully connected) 
    <ul>
      <li>Each independent path (segment) is parallelizable!</li>
    </ul>
  </li>
  <li>Automatic tuning, pruning, evolving is all parallelizable!
    <ul>
      <li>Just train each NN separately</li>
      <li>Check back in with Prefrontal to “compete”</li>
    </ul>
  </li>
</ul>

</section><section>

<p>Structure you can play with (textbook)</p>

<ul>
  <li>limit connections </li>
</ul>

<p>jargon: <em>receptive fields</em></p>

<ul>
  <li>limit weights </li>
</ul>

<p>jargon: <em>weight sharing</em></p>

<p>All the rage: <em>convolutional networks</em></p>

</section><section>

<p>Unconventional structure to play with</p>

<p>New ideas, no jargon yet, just crackpot names</p>

<ul>
  <li>limit weight ranges (e.g. -1 to 1, 0 to 1, etc)</li>
  <li>weight “snap to grid” (snap learning)</li>
</ul>

</section><section>

<p>Joke: “What’s the difference between a scientist and a crackpot?”</p>

</section><section>

<p>Ans: “P-value”</p>

<ul>
  <li>High-<strong>P</strong>robability null hypothesis</li>
  <li>Not <strong>P</strong>ublished</li>
  <li>Not <strong>P</strong>eer-reviewed</li>
  <li>No <strong>P</strong>yPi <strong>p</strong>ackage</li>
</ul>

<p>I’m a crackpot!</p>

</section><section>

<p>Resources</p>

<ul>
  <li><a href="http://keras.io/">keras.io</a>: Scalable Python NNs</li>
  <li><a href="http://hagan.okstate.edu/NNDesign.pdf">Neural Network Design</a>: Free NN Textbook!</li>
  <li><a href="https://github.com/hobson/pug-ann">pug-ann</a>: Helpers for PyBrain and Wunderground</li>
  <li><a href="https://github.com/pybrain2/pybrain2">PyBrain2</a>: We’re working on it</li>
</ul>

</section><section>

<p>Code highlighting test</p>

<div class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">function</span> <span class="nx">linkify</span><span class="p">(</span> <span class="nx">selector</span> <span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span><span class="p">(</span> <span class="nx">supports3DTransforms</span> <span class="p">)</span> <span class="p">{</span>
 
    <span class="kd">var</span> <span class="nx">nodes</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">querySelectorAll</span><span class="p">(</span> <span class="nx">selector</span> <span class="p">);</span>
 
    <span class="k">for</span><span class="p">(</span> <span class="kd">var</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="nx">len</span> <span class="o">=</span> <span class="nx">nodes</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&amp;</span><span class="nx">lt</span><span class="p">;</span> <span class="nx">len</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
      <span class="kd">var</span> <span class="nx">node</span> <span class="o">=</span> <span class="nx">nodes</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span>
 
      <span class="k">if</span><span class="p">(</span> <span class="o">!</span><span class="nx">node</span><span class="p">.</span><span class="nx">className</span> <span class="p">)</span> <span class="p">{</span>
        <span class="nx">node</span><span class="p">.</span><span class="nx">className</span> <span class="o">+=</span> <span class="s1">&#39; roll&#39;</span><span class="p">;</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

</section>


            </div class="slides">

    </div class="reveal">
    <script src="/reveal.js/lib/js/head.min.js"></script>
    <script src="/reveal.js/js/reveal.js"></script>

    <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

                

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: '/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: '/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: '/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: '/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: '/reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    
          { src: '/reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
                    
                    
        ]
      });

    </script>

  </body>
</html>
